# Learning Experiments

*What we're testing, discovering, and questioning about how humans learn with AI*

**Note**: The experiments below are hypothetical examples showing the types of experiments we plan to run. They will be replaced with actual experiments and real data as we implement and test them.

## Current Status: Living Laboratory

This isn't a product roadmap. It's a research journal. Everything here is a hypothesis being tested.

## üß™ Example Active Experiments (Hypothetical)

### Example Experiment #1: "Thinking-First Protocol" (Hypothetical)
**Hypothesis**: Forcing 2 minutes of unassisted thinking before AI responds improves understanding
**Testing Now**: Timer-locked responses
**Measuring**: Depth of follow-up questions, time to "aha moment"
**Early Signal**: Students initially frustrated, then report deeper insights
**Surprises**: Some students hack around it - that's data too

### Example Experiment #2: "Confusion Optimization" (Hypothetical)
**Hypothesis**: There's an optimal level of productive confusion
**Testing Now**: AI adjusts ambiguity based on frustration signals
**Measuring**: Breakthrough velocity after confusion periods
**Early Signal**: Too much clarity might actually harm learning
**Surprises**: Confusion tolerance varies wildly by time of day

### Example Experiment #3: "Teaching Mode" (Hypothetical)
**Hypothesis**: Explaining to AI deepens understanding more than receiving explanations
**Testing Now**: AI plays confused student, asks clarifying questions
**Measuring**: Concept retention after 48 hours
**Early Signal**: Students catch their own misconceptions while explaining
**Surprises**: Some students teach incorrectly but confidently - valuable data

### Example Experiment #4: "Connection Mapping" (Hypothetical)
**Hypothesis**: Cross-domain connections accelerate mastery
**Testing Now**: AI finds unexpected links between different subjects
**Measuring**: Novel application of concepts in unrelated contexts
**Early Signal**: CS students understanding recursion through biology examples
**Surprises**: Sometimes wrong connections lead to right insights

## üî¨ Example Queued Experiments (Hypothetical Ideas)

### "Struggle Fingerprinting"
Every person has a unique pattern of productive struggle. Can we identify and optimize for it?

### "Collective Confusion"
When 100 students get stuck at the same concept, what patterns emerge?

### "Anti-Teaching"
What if AI deliberately teaches something wrong first, then helps student discover the error?

### "Learning Velocity Curves"
Does learning speed increase exponentially with practice, or are there plateaus?

### "Peer Confusion Matching"
Match students with complementary confusion patterns - does teaching each other help?

## üíÄ Example Failed Experiments (Hypothetical Graveyard)

### "Always Available Help" (Killed Week 1)
**Hypothesis**: Instant AI help improves learning
**Result**: Creates learned helplessness
**Learning**: Delay might be a feature, not a bug

### "Comprehensive Explanations" (Killed Week 2)
**Hypothesis**: More complete explanations = better understanding
**Result**: Information overload, shallow processing
**Learning**: Incompleteness forces active thinking

## üé≤ Wild Ideas (Not Yet Experiments)

- What if AI got "tired" and gave worse help over time, forcing independence?
- What if wrong answers were sometimes marked correct to test confidence?
- What if learning paths were intentionally non-linear?
- What if AI pretended to learn alongside the student?
- What if frustration was gamified as XP points?
- What if forgetting was intentionally induced to strengthen re-learning?

## üìä Example Unexpected Discoveries (Hypothetical)

### Example Discovery #1: "The 3 AM Effect" (Hypothetical)
Students learning between 2-4 AM show different cognitive patterns - more creative, less linear thinking. Should AI adapt to circadian rhythms?

### Example Discovery #2: "Explanation Fatigue" (Hypothetical)
After 3 explanations of the same concept, success rate drops to near zero. Better to change domains entirely and circle back.

### Example Discovery #3: "The Rubber Duck Paradox" (Hypothetical)
Students who explain to AI before asking questions solve problems 40% faster - even when AI says nothing.

## üîç What We're Measuring (And Not Sure Why)

- Time between questions
- Confidence in wrong answers
- Speed of abandoning approaches
- Linguistic complexity evolution
- Parenthetical thought patterns
- Correction acceptance velocity
- Metaphor generation rate
- Silence tolerance threshold

## ü§î Questions We Can't Answer Yet

1. Is struggling with AI different from struggling alone?
2. Does AI assistance change the brain's learning pathways permanently?
3. Is there a "cognitive uncanny valley" where AI is helpful enough to use but harmful enough to hurt?
4. Can collective confusion be more valuable than collective intelligence?
5. Is the goal to need AI less over time, or to collaborate better over time?

## üìà Metrics That Might Matter (Or Might Not)

- **Insight Density**: Aha moments per hour
- **Confusion Recovery Time**: How fast from stuck to unstuck
- **Transfer Distance**: How far can concepts be applied from origin
- **Unlearning Speed**: How fast can wrong models be replaced
- **Question Quality Score**: Are questions getting better?
- **Certainty Volatility**: How often confidence changes

## üöÄ How to Contribute Experiments

Have a hypothesis about learning? Propose an experiment:

1. **Hypothesis**: What do you think happens?
2. **Method**: How would we test it?
3. **Metrics**: What would we measure?
4. **Duration**: How long to run it?
5. **Failure Criteria**: How would we know it failed?

Submit via issue labeled "experiment-proposal"

## üéØ Meta-Experiment

This document itself is an experiment. Does transparency about experimentation change how people learn? Does knowing you're part of an experiment affect outcomes? We're measuring engagement with this document as a signal.

## üìù Example Weekly Learning Log (Hypothetical Format)

### Example Week Entry
- These are hypothetical observations
- Showing format for future real entries
- Will be replaced with actual findings
- From real user testing

---

*This document will be updated with real experiments and findings as we test with actual users. The hypothetical examples above show the experimental approach we'll take.*

**Core Philosophy**: We don't know how humans best learn with AI. We're discovering it together.

**Reminder**: All experiments and discoveries listed above are hypothetical examples. Real experiments will replace these as we begin actual testing.